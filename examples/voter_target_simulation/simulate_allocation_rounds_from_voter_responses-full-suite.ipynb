{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from darts.allocation import Allocator\n",
    "from darts.bandit import Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed in 0.022923946380615234 seconds\n"
     ]
    }
   ],
   "source": [
    "# load in the data for the target pool\n",
    "start = time.time()\n",
    "file_loc = 'data/'\n",
    "allocation_pool_df = pd.read_csv(file_loc + 'target_voter_universe.csv')\n",
    "print(f\"... completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>probability</th>\n",
       "      <th>target_round</th>\n",
       "      <th>target_result</th>\n",
       "      <th>target_reward</th>\n",
       "      <th>target_regret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>model1_rf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>model2_lr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>model3_lr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>model4_xgb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>model5_lgbm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>model_baseline</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>model1_rf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>model2_lr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>model3_lr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>model4_xgb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_id        model_id  probability  target_round  target_result  \\\n",
       "0          0       model1_rf          0.0             0              0   \n",
       "1          0       model2_lr          1.0             0              0   \n",
       "2          0       model3_lr          0.0             0              0   \n",
       "3          0      model4_xgb          1.0             0              0   \n",
       "4          0     model5_lgbm          0.0             0              0   \n",
       "5          0  model_baseline          1.0             0              0   \n",
       "6          1       model1_rf          0.0             0              0   \n",
       "7          1       model2_lr          0.0             0              0   \n",
       "8          1       model3_lr          0.0             0              0   \n",
       "9          1      model4_xgb          0.0             0              0   \n",
       "\n",
       "   target_reward  target_regret  \n",
       "0              0              0  \n",
       "1              0              0  \n",
       "2              0              0  \n",
       "3              0              0  \n",
       "4              0              0  \n",
       "5              0              0  \n",
       "6              0              0  \n",
       "7              0              0  \n",
       "8              0              0  \n",
       "9              0              0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocation_pool_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.54"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(allocation_pool_df.shape[0]/6)/250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed in 0.007577180862426758 seconds\n"
     ]
    }
   ],
   "source": [
    "# load in our target results\n",
    "start = time.time()\n",
    "file_loc = 'data/'\n",
    "target_results_df = pd.read_csv(file_loc + 'voter_responses.csv')\n",
    "print(f\"... completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_id  target_result\n",
       "0          0              1\n",
       "1          1              0\n",
       "2          3              0\n",
       "3          4              0\n",
       "4         16              1\n",
       "5         22              0\n",
       "6         26              0\n",
       "7         27              1\n",
       "8         29              0\n",
       "9         31              0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column that indicates the id of an individual target\n",
    "target_id_col = 'target_id'\n",
    "\n",
    "# Specify the column that provides the confidence ranking (e.g. probability)\n",
    "# that any of the models have on this target resulting in a 'reward'\n",
    "# (e.g. the target outcome of the model)\n",
    "confidence_rank_col = 'probability'\n",
    "\n",
    "# Specify the column containing the names of the different 'arms' or\n",
    "# models we're using in the target pool\n",
    "arm_name_col = 'model_id'\n",
    "\n",
    "# Specify column to track the pool round\n",
    "pool_round_col = 'target_round'\n",
    "\n",
    "# Specify the column that indicates the 'result'\n",
    "result_col = 'target_result'\n",
    "\n",
    "# Specify the column that indicates the 'reward'\n",
    "reward_col = 'target_reward'\n",
    "\n",
    "# Specify the column that indicates the 'regret'\n",
    "regret_col = 'target_regret'\n",
    "\n",
    "# Specify the column that indicates which arm was 'picked'\n",
    "picked_col = 'model_picked'\n",
    "\n",
    "# Specify an initial allocation method across the pool of models. These must sum to 1.\n",
    "# For the simulation, we'll start with an equal allocation for each model.\n",
    "allocation_method = {\n",
    "    'model_baseline': (1/6),\n",
    "    'model1_rf': (1/6),\n",
    "    'model2_lr': (1/6),\n",
    "    'model3_lr': (1/6),\n",
    "    'model4_xgb': (1/6),\n",
    "    'model5_lgbm': (1/6)\n",
    "}\n",
    "\n",
    "# Specify number of rounds to simulate\n",
    "simulation_rounds = 50\n",
    "\n",
    "# Specify the number of targets to pull in the first round.\n",
    "num_targets = 250\n",
    "\n",
    "# Specify the allocation policy\n",
    "allocation_policy = 'UCB1'\n",
    "\n",
    "# Specify the allocation strategy\n",
    "allocation_strategy = 'round-robin'\n",
    "\n",
    "# Specify the allocation order\n",
    "allocation_order = 'best'\n",
    "\n",
    "# bandit parameters\n",
    "ucb_scale = 1.96\n",
    "epsilon = 0.1\n",
    "greed_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the file of responses for updating of results\n",
    "target_results_df = target_results_df.set_index([target_id_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = {\n",
    "    1: {'policy': 'UCB1',\n",
    "        'options': {'greed_factor': [1, 1.5, 3]},\n",
    "        'allocations': [('round-robin', 'best'), ('round-robin', 'random'), ('greedy', 'best')]\n",
    "       },\n",
    "    2: {'policy': 'Bayes_UCB',\n",
    "        'options': {'ucb_scale': [1, 2, 3],\n",
    "                    'greed_factor': [1, 1.5, 3]},\n",
    "        'allocations': [('round-robin', 'best'), ('round-robin', 'random'), ('greedy', 'best')]\n",
    "       },\n",
    "    3: {'policy': 'epsilon_greedy',\n",
    "        'options': {'epsilon': [0.1, 0.35, 0.55],\n",
    "                    'greed_factor': [1, 1.5, 3]},\n",
    "        'allocations': [('round-robin', 'best'), ('round-robin', 'random'), ('greedy', 'best')]\n",
    "       }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_perm_idx(idx, policy, ucb_scale, epsilon, greed_factor, strategy, order, pool_size, num_rounds):\n",
    "    return str(tuple(make_perm_dict(idx, policy, ucb_scale, epsilon, greed_factor, strategy, order, pool_size, num_rounds).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_perm_dict(idx, policy, ucb_scale, epsilon, greed_factor, strategy, order, pool_size, num_rounds):\n",
    "    return {\n",
    "        'perm_idx': idx,\n",
    "        'policy': policy,\n",
    "        'ucb_scale': ucb_scale,\n",
    "        'epsilon': epsilon,\n",
    "        'greed_factor': greed_factor,\n",
    "        'strategy': strategy,\n",
    "        'order': order,\n",
    "        'pool_size': pool_size,\n",
    "        'num_rounds': num_rounds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_simulation(allocation_pool_df, target_results_df, allocation_method, allocation_policy, ucb_scale, epsilon, greed_factor, allocation_strategy, allocation_order, num_targets, simulation_rounds):\n",
    "    # Set up defaults - nothing picked yet\n",
    "    allocation_pool_df[picked_col] = 0\n",
    "    allocation_pool_df[pool_round_col] = 0\n",
    "    allocation_pool_df[reward_col] = 0\n",
    "    allocation_pool_df[regret_col] = 0\n",
    "\n",
    "    timesteps = []\n",
    "\n",
    "    for allocation_round in range(1, simulation_rounds+1):\n",
    "\n",
    "        remaining_allocation_pool_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col]==0])\n",
    "\n",
    "        # set up an allocator\n",
    "        # TODO: I think this should be in the sequence of:\n",
    "        #  allocation_pool_df, arm_name_col, target_id_col, confidence_rank_col, allocation_method, strategy, order\n",
    "        # TODO: Would be good to explain that Allocator introduces a picked column to the dataframe. This is masked behavior.\n",
    "        #       I may rely on this column if I know about it, but not sure if it will ever go away?\n",
    "        allocator = Allocator(allocation_method, num_targets, remaining_allocation_pool_df, arm_name_col,\n",
    "                              confidence_rank_col, target_id_col, strategy=allocation_strategy,\n",
    "                              order=allocation_order) # another idea is to provide a dictionary mapping to simplify the calling interface\n",
    "\n",
    "        # retreive targets\n",
    "        targets = allocator.allocate_pool() # TODO: can we change this to allocator.retrieve_targets(num_targets) ? Also, might be nice to have a DF return option\n",
    "\n",
    "        # convert to dataframe\n",
    "        targets_df = pd.DataFrame(targets, columns=[target_id_col, arm_name_col])\n",
    "        targets_df[pool_round_col] = allocation_round\n",
    "        targets_df = targets_df[[target_id_col, pool_round_col]]\n",
    "\n",
    "        # join targets with universe of allocations\n",
    "        targets_df = targets_df.set_index([target_id_col])\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col])\n",
    "        allocation_pool_df.update(targets_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()   \n",
    "\n",
    "        # add indicator for the arm we picked\n",
    "        target_arm_picked_df = pd.DataFrame(targets, columns=[target_id_col, arm_name_col])\n",
    "        target_arm_picked_df[picked_col] = 1\n",
    "        target_arm_picked_df = target_arm_picked_df[[target_id_col, arm_name_col, picked_col]]\n",
    "\n",
    "        # join target model we picked with the universe of allocations\n",
    "        target_arm_picked_df = target_arm_picked_df.set_index([target_id_col, arm_name_col])\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col, arm_name_col])\n",
    "        allocation_pool_df.update(target_arm_picked_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()\n",
    "\n",
    "        # evaluate this allocation pool with the bandit\n",
    "        last_allocation_pool_df = allocation_pool_df[allocation_pool_df[pool_round_col] == allocation_round]\n",
    "\n",
    "        # sync up results\n",
    "        last_allocation_pool_df = last_allocation_pool_df.set_index([target_id_col])\n",
    "        last_allocation_pool_df.update(target_results_df)\n",
    "        last_allocation_pool_df = last_allocation_pool_df.reset_index(level=0)\n",
    "\n",
    "        # update results in the master pool set\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col, arm_name_col, confidence_rank_col, pool_round_col, reward_col, regret_col, picked_col])\n",
    "        last_allocation_pool_df = last_allocation_pool_df.set_index([target_id_col, arm_name_col, confidence_rank_col, pool_round_col, reward_col, regret_col, picked_col])\n",
    "        allocation_pool_df.update(last_allocation_pool_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()\n",
    "\n",
    "        TP = allocation_pool_df[result_col] * allocation_pool_df[confidence_rank_col] * allocation_pool_df[picked_col]\n",
    "        TN = (1 - allocation_pool_df[result_col]) * (1 - allocation_pool_df[confidence_rank_col]) * (1 - allocation_pool_df[picked_col])\n",
    "        FN = allocation_pool_df[result_col] * (1 - allocation_pool_df[confidence_rank_col]) * (1 - allocation_pool_df[picked_col])\n",
    "        FP = (1 - allocation_pool_df[result_col]) * allocation_pool_df[confidence_rank_col] * allocation_pool_df[picked_col]\n",
    "\n",
    "        # update rewards from the results we synced\n",
    "        allocation_pool_df[reward_col] = TP\n",
    "\n",
    "        # update regrets from the results we synced\n",
    "        # regrets can be false positives or false negatives (type 1 and type 2 error)\n",
    "        allocation_pool_df[regret_col] = FP\n",
    "\n",
    "        # prepare a dataframe with our results from this round and all prior rounds for the bandit to evaluate\n",
    "        results_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col] != 0])\n",
    "\n",
    "        if results_df[(results_df[result_col]==1)&(results_df[reward_col]==1)].shape[0] == 0:\n",
    "            # no more results left after a steady state - reset selections to include possibilities from the other models\n",
    "\n",
    "            TP = allocation_pool_df[result_col] * allocation_pool_df[confidence_rank_col]\n",
    "            TN = (1 - allocation_pool_df[result_col]) * (1 - allocation_pool_df[confidence_rank_col])\n",
    "            FN = allocation_pool_df[result_col] * (1 - allocation_pool_df[confidence_rank_col])\n",
    "            FP = (1 - allocation_pool_df[result_col]) * allocation_pool_df[confidence_rank_col]\n",
    "\n",
    "            # update rewards from the results we synced\n",
    "            allocation_pool_df[reward_col] = TP\n",
    "\n",
    "            # update regrets from the results we synced\n",
    "            # regrets can be false positives or false negatives (type 1 and type 2 error)\n",
    "            allocation_pool_df[regret_col] = FN\n",
    "\n",
    "            # prepare a dataframe with just our results of this round for the bandit to evaluate\n",
    "            results_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col] == allocation_round])\n",
    "\n",
    "\n",
    "        # prepare dictionary of results for timestep tracking\n",
    "        stats = results_df[[arm_name_col, reward_col, regret_col]].groupby(arm_name_col).agg({reward_col:['count','sum'],\n",
    "                                                                                              regret_col:['sum']})\n",
    "        stats.columns = stats.columns.to_flat_index()\n",
    "        stats.columns = ['_'.join(tup).rstrip('_') for tup in stats.columns.values]\n",
    "        stats = pd.DataFrame(stats).reset_index()\n",
    "        stats.rename(columns={'target_reward_count': 'reward_evaluation_size', 'target_reward_sum': 'rewards', 'target_regret_sum':'regrets'}, inplace=True)\n",
    "        stats = stats.set_index([arm_name_col])\n",
    "        stats['allocation'] = 0.0\n",
    "        allocs = pd.DataFrame.from_dict(allocation_method, orient='index', columns = ['allocation'])\n",
    "        allocs.index.name=arm_name_col\n",
    "        stats.update(allocs)\n",
    "        stats = stats.reset_index()\n",
    "\n",
    "        timesteps.append(stats.to_dict(orient='records'))\n",
    "\n",
    "        # set up a multi-arm bandit and calculate allocations to each arm.\n",
    "        bandit = Bandit(results_df, arm_name_col, reward_col, regret_col, policy = allocation_policy, t = allocation_round, ucb_scale = ucb_scale, epsilon = epsilon, greed_factor = greed_factor)\n",
    "\n",
    "        # use these allocations for the next round\n",
    "        allocation_method = bandit.get_new_allocations()# bandit.make_allocs().set_index(arm_name_col)['allocation'].to_dict()\n",
    "        \n",
    "    print(\"Allocations after round\", allocation_round, \":\\n\", allocation_method)\n",
    "    #display(bandit.get_allocation_stats())\n",
    "    \n",
    "    return timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.237623775645302, 'model5_lgbm': 0.17852678602966668, 'model2_lr': 0.16799011346253792, 'model1_rf': 0.15730073549588555, 'model_baseline': 0.13225704997401422, 'model4_xgb': 0.12630153939259361}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.4540191965701357, 'model2_lr': 0.1786026719384297, 'model5_lgbm': 0.1283215032877796, 'model1_rf': 0.09000677053335682, 'model4_xgb': 0.0774178584300948, 'model_baseline': 0.07163199924020333}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9891237408239338, 'model2_lr': 0.0028281661167956053, 'model_baseline': 0.0024444335969012717, 'model4_xgb': 0.0022170742185889276, 'model5_lgbm': 0.0018055360570459794, 'model1_rf': 0.0015810491867343278}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.33442584147350923, 'model5_lgbm': 0.1829887505942427, 'model1_rf': 0.1502649628174927, 'model2_lr': 0.12064635198306202, 'model_baseline': 0.11133082115610396, 'model3_lr': 0.10034327197558937}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.6721896859160006, 'model5_lgbm': 0.16062364026909934, 'model1_rf': 0.05374945227195585, 'model2_lr': 0.04530402486149295, 'model3_lr': 0.03520158369869996, 'model_baseline': 0.03293161298275142}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9990282231732546, 'model2_lr': 0.00024737205254839564, 'model5_lgbm': 0.00021184324975661252, 'model_baseline': 0.00017989039674055988, 'model1_rf': 0.0001707488184704997, 'model3_lr': 0.00016192230922924098}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.25713080152254714, 'model5_lgbm': 0.19163018742436533, 'model1_rf': 0.18140048778665394, 'model2_lr': 0.13269490294441616, 'model_baseline': 0.11986960787624072, 'model4_xgb': 0.11727401244577662}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.4670344440185371, 'model1_rf': 0.18767759199368755, 'model5_lgbm': 0.17125147602106372, 'model4_xgb': 0.06750948572000921, 'model_baseline': 0.060043065314717266, 'model2_lr': 0.04648393693198522}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9907946703799833, 'model1_rf': 0.0021743868812528757, 'model5_lgbm': 0.0021743868812528757, 'model4_xgb': 0.0020562799533618403, 'model_baseline': 0.0014180804480501716, 'model2_lr': 0.0013821954560989903}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.30224413667973105, 'model2_lr': 0.18380713949028485, 'model5_lgbm': 0.17381974009051668, 'model1_rf': 0.13956033781471142, 'model4_xgb': 0.10391245992911848, 'model_baseline': 0.0966561859956375}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6557438534191761, 'model2_lr': 0.21706129104232272, 'model5_lgbm': 0.045627979228398306, 'model_baseline': 0.04259621165430607, 'model4_xgb': 0.024456931635625512, 'model1_rf': 0.01451373302017132}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9997761871630396, 'model2_lr': 7.762098771462154e-05, 'model_baseline': 7.762098771462148e-05, 'model4_xgb': 4.044607552121534e-05, 'model5_lgbm': 1.9795999867551252e-05, 'model1_rf': 8.328786142245804e-06}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.28811532843760207, 'model2_lr': 0.18743712076622304, 'model5_lgbm': 0.17397566053363944, 'model1_rf': 0.13950074245155467, 'model4_xgb': 0.11367028747749057, 'model_baseline': 0.09730086033349028}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6577865120269586, 'model2_lr': 0.13987371069615012, 'model_baseline': 0.08656979378349064, 'model5_lgbm': 0.05337962893814955, 'model4_xgb': 0.0326648580833912, 'model1_rf': 0.029725496471859958}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9997343652754783, 'model2_lr': 8.745097789859343e-05, 'model_baseline': 6.863028015203866e-05, 'model4_xgb': 6.577519823536619e-05, 'model5_lgbm': 3.0003383537514394e-05, 'model1_rf': 1.377488469810369e-05}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.2777035059403163, 'model5_lgbm': 0.17205008234257346, 'model2_lr': 0.16325386759570729, 'model1_rf': 0.16307421548537884, 'model4_xgb': 0.11650311622652054, 'model_baseline': 0.10741521240950358}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.5834016717952001, 'model2_lr': 0.21089515138331835, 'model5_lgbm': 0.08110737139973065, 'model1_rf': 0.048988514215820615, 'model_baseline': 0.04322493334250438, 'model4_xgb': 0.03238235786342583}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9995963366067934, 'model2_lr': 0.00013945495769755, 'model_baseline': 0.0001008507587136951, 'model4_xgb': 0.00010085075871369503, 'model5_lgbm': 4.11682982485005e-05, 'model1_rf': 2.133861983324565e-05}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.5685721290953597, 'model5_lgbm': 0.22853855026077718, 'model_baseline': 0.07538904214458478, 'model2_lr': 0.05184003702414197, 'model1_rf': 0.05094231082010691, 'model3_lr': 0.02471793065502952}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9882836620729301, 'model1_rf': 0.004442657188964541, 'model5_lgbm': 0.003776781635462278, 'model_baseline': 0.002844517829382719, 'model2_lr': 0.000448296508601642, 'model3_lr': 0.0002040847646587978}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999020883418726, 'model1_rf': 8.771921620335847e-05, 'model5_lgbm': 8.47699585674253e-06, 'model2_lr': 1.5809613952931247e-06, 'model_baseline': 1.2340851409626087e-07, 'model3_lr': 1.1076157815130753e-08}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.5842131234358997, 'model5_lgbm': 0.17172189218588213, 'model2_lr': 0.0858808105021106, 'model1_rf': 0.08442466903061509, 'model_baseline': 0.04530705585249533, 'model3_lr': 0.028452448992997204}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9435427604811778, 'model5_lgbm': 0.04455626532342182, 'model2_lr': 0.005012070948340281, 'model_baseline': 0.0030136402298778894, 'model1_rf': 0.0028029009445280687, 'model3_lr': 0.0010723620726539368}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999497315280307, 'model5_lgbm': 4.842336073870294e-05, 'model1_rf': 6.949650417849299e-07, 'model2_lr': 6.949650417849299e-07, 'model3_lr': 3.4522115553009924e-07, 'model_baseline': 1.0995999158156307e-07}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.5134655143285813, 'model5_lgbm': 0.2140196419265718, 'model2_lr': 0.11000283443144133, 'model1_rf': 0.08309589433426132, 'model_baseline': 0.05325042628388476, 'model3_lr': 0.026165688695259502}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9733265709738808, 'model5_lgbm': 0.014643393707322433, 'model1_rf': 0.0041556700685835895, 'model_baseline': 0.003422570260124477, 'model2_lr': 0.00314044676391815, 'model3_lr': 0.0013113482261705382}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999884669384153, 'model2_lr': 5.793195900922363e-06, 'model5_lgbm': 2.9356987860380747e-06, 'model1_rf': 1.7917703738784135e-06, 'model_baseline': 5.695330661112184e-07, 'model3_lr': 4.428634576622139e-07}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.3484634167062178, 'model1_rf': 0.2117592474008226, 'model5_lgbm': 0.20818327883763335, 'model4_xgb': 0.08956409944644979, 'model_baseline': 0.08120462800403787, 'model2_lr': 0.06082532960483856}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6937692728579479, 'model1_rf': 0.15266065457804584, 'model5_lgbm': 0.10354805067788819, 'model4_xgb': 0.032814663837657736, 'model_baseline': 0.01110404276453679, 'model2_lr': 0.006103315283923445}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9999003419480211, 'model1_rf': 4.185388631650222e-05, 'model4_xgb': 2.774673863416687e-05, 'model5_lgbm': 2.6242579666458866e-05, 'model_baseline': 2.6850283678171033e-06, 'model2_lr': 1.129818994021255e-06}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.3284192313341858, 'model5_lgbm': 0.2079118984836885, 'model1_rf': 0.2033554356987397, 'model_baseline': 0.09085255854972363, 'model4_xgb': 0.08584450148834451, 'model2_lr': 0.08361637444531798}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6263302307229658, 'model1_rf': 0.18240727287156852, 'model5_lgbm': 0.13514387307345802, 'model4_xgb': 0.03244976011540041, 'model_baseline': 0.01755904504984093, 'model2_lr': 0.006109818166766373}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9998498896799636, 'model1_rf': 5.453095901379305e-05, 'model4_xgb': 4.5162969713662036e-05, 'model5_lgbm': 4.299625536074969e-05, 'model_baseline': 4.594012152323152e-06, 'model2_lr': 2.8261237958652255e-06}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.31140869315612285, 'model5_lgbm': 0.20282378127068545, 'model1_rf': 0.1874363366433287, 'model2_lr': 0.11859844014836088, 'model_baseline': 0.09545804829610219, 'model4_xgb': 0.08427470048540002}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.5793981952868471, 'model1_rf': 0.2052341898639487, 'model5_lgbm': 0.15709305846332094, 'model4_xgb': 0.031316545477464576, 'model_baseline': 0.019136878373950318, 'model2_lr': 0.007821132534468204}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9997795564801796, 'model1_rf': 8.500206346102946e-05, 'model5_lgbm': 6.592262556203881e-05, 'model4_xgb': 5.4865012834255594e-05, 'model_baseline': 8.793318024038861e-06, 'model2_lr': 5.860499938880978e-06}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model4_xgb': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9837064118022794, 'model1_rf': 0.003258717639544145, 'model2_lr': 0.003258717639544145, 'model4_xgb': 0.003258717639544145, 'model5_lgbm': 0.003258717639544145, 'model_baseline': 0.003258717639544145}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9999451333260178, 'model1_rf': 1.097333479644464e-05, 'model2_lr': 1.097333479644464e-05, 'model4_xgb': 1.097333479644464e-05, 'model5_lgbm': 1.097333479644464e-05, 'model_baseline': 1.097333479644464e-05}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6500000000000001, 'model1_rf': 0.07, 'model2_lr': 0.07, 'model4_xgb': 0.07, 'model5_lgbm': 0.07, 'model_baseline': 0.07}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.8498312028095432, 'model1_rf': 0.03003375943809133, 'model2_lr': 0.03003375943809133, 'model4_xgb': 0.03003375943809133, 'model5_lgbm': 0.03003375943809133, 'model_baseline': 0.03003375943809133}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9937938771079107, 'model1_rf': 0.0012412245784178906, 'model2_lr': 0.0012412245784178906, 'model4_xgb': 0.0012412245784178906, 'model5_lgbm': 0.0012412245784178906, 'model_baseline': 0.0012412245784178906}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.45, 'model1_rf': 0.11000000000000003, 'model2_lr': 0.11000000000000003, 'model4_xgb': 0.11000000000000003, 'model5_lgbm': 0.11000000000000003, 'model_baseline': 0.11000000000000003}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6233314819933395, 'model1_rf': 0.07533370360133207, 'model2_lr': 0.07533370360133207, 'model4_xgb': 0.07533370360133207, 'model5_lgbm': 0.07533370360133207, 'model_baseline': 0.07533370360133207}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9319390468398446, 'model1_rf': 0.0136121906320311, 'model2_lr': 0.0136121906320311, 'model4_xgb': 0.0136121906320311, 'model5_lgbm': 0.0136121906320311, 'model_baseline': 0.0136121906320311}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9837064118022794, 'model1_rf': 0.003258717639544145, 'model2_lr': 0.003258717639544145, 'model3_lr': 0.003258717639544145, 'model5_lgbm': 0.003258717639544145, 'model_baseline': 0.003258717639544145}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999451333260178, 'model1_rf': 1.097333479644464e-05, 'model2_lr': 1.097333479644464e-05, 'model3_lr': 1.097333479644464e-05, 'model5_lgbm': 1.097333479644464e-05, 'model_baseline': 1.097333479644464e-05}\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 0.6500000000000001, 'model1_rf': 0.07, 'model2_lr': 0.07, 'model3_lr': 0.07, 'model4_xgb': 0.07, 'model_baseline': 0.07}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.8498312028095434, 'model1_rf': 0.030033759438091334, 'model2_lr': 0.030033759438091334, 'model3_lr': 0.030033759438091334, 'model5_lgbm': 0.030033759438091334, 'model_baseline': 0.030033759438091334}\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 0.9937938771079107, 'model1_rf': 0.0012412245784178906, 'model2_lr': 0.0012412245784178906, 'model3_lr': 0.0012412245784178906, 'model4_xgb': 0.0012412245784178906, 'model_baseline': 0.0012412245784178906}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.44999999999999996, 'model1_rf': 0.11000000000000001, 'model2_lr': 0.11000000000000001, 'model3_lr': 0.11000000000000001, 'model5_lgbm': 0.11000000000000001, 'model_baseline': 0.11000000000000001}\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.6233314819933395, 'model1_rf': 0.07533370360133207, 'model2_lr': 0.07533370360133207, 'model3_lr': 0.07533370360133207, 'model5_lgbm': 0.07533370360133207, 'model_baseline': 0.07533370360133207}\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 0.9319390468398444, 'model1_rf': 0.013612190632031099, 'model2_lr': 0.013612190632031099, 'model3_lr': 0.013612190632031099, 'model4_xgb': 0.013612190632031099, 'model_baseline': 0.013612190632031099}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model4_xgb': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9837064118022794, 'model1_rf': 0.003258717639544145, 'model2_lr': 0.003258717639544145, 'model4_xgb': 0.003258717639544145, 'model5_lgbm': 0.003258717639544145, 'model_baseline': 0.003258717639544145}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9999451333260178, 'model1_rf': 1.097333479644464e-05, 'model2_lr': 1.097333479644464e-05, 'model4_xgb': 1.097333479644464e-05, 'model5_lgbm': 1.097333479644464e-05, 'model_baseline': 1.097333479644464e-05}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6500000000000001, 'model1_rf': 0.07, 'model2_lr': 0.07, 'model4_xgb': 0.07, 'model5_lgbm': 0.07, 'model_baseline': 0.07}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.8498312028095432, 'model1_rf': 0.03003375943809133, 'model2_lr': 0.03003375943809133, 'model4_xgb': 0.03003375943809133, 'model5_lgbm': 0.03003375943809133, 'model_baseline': 0.03003375943809133}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9937938771079107, 'model1_rf': 0.0012412245784178906, 'model2_lr': 0.0012412245784178906, 'model4_xgb': 0.0012412245784178906, 'model5_lgbm': 0.0012412245784178906, 'model_baseline': 0.0012412245784178906}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.45, 'model1_rf': 0.11000000000000003, 'model2_lr': 0.11000000000000003, 'model4_xgb': 0.11000000000000003, 'model5_lgbm': 0.11000000000000003, 'model_baseline': 0.11000000000000003}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6233314819933395, 'model1_rf': 0.07533370360133207, 'model2_lr': 0.07533370360133207, 'model4_xgb': 0.07533370360133207, 'model5_lgbm': 0.07533370360133207, 'model_baseline': 0.07533370360133207}\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9319390468398446, 'model1_rf': 0.0136121906320311, 'model2_lr': 0.0136121906320311, 'model4_xgb': 0.0136121906320311, 'model5_lgbm': 0.0136121906320311, 'model_baseline': 0.0136121906320311}\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "simulations = []\n",
    "for _, v in permutations.items():\n",
    "    policy = v['policy']\n",
    "    ucb_scale = None\n",
    "    epsilon = None\n",
    "    greed_factor = None\n",
    "    if policy == 'UCB1':\n",
    "        for strategy, order in v['allocations']:\n",
    "            for greed_factor in v['options']['greed_factor']:\n",
    "                timesteps = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                               ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                payload = make_perm_dict(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                payload['timesteps'] = timesteps\n",
    "                simulations.append(payload)\n",
    "                i = i + 1\n",
    "    if policy == 'Bayes_UCB':\n",
    "        for strategy, order in v['allocations']:\n",
    "            for ucb_scale in v['options']['ucb_scale']:\n",
    "                for greed_factor in v['options']['greed_factor']:\n",
    "                    timesteps = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                                   ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    payload = make_perm_dict(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    payload['timesteps'] = timesteps\n",
    "                    simulations.append(payload)\n",
    "                    i = i + 1\n",
    "    if policy == 'epsilon_greedy':\n",
    "        for strategy, order in v['allocations']:\n",
    "            for epsilon in v['options']['epsilon']:\n",
    "                for greed_factor in v['options']['greed_factor']:\n",
    "                    timesteps = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                                   ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    payload = make_perm_dict(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    payload['timesteps'] = timesteps\n",
    "                    simulations.append(payload)\n",
    "                    i = i + 1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write simulation sequence to a json file\n",
    "with open('simulations.json', 'w') as json_file:\n",
    "    json.dump(simulations, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prior version that uses dictionary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_simulation(allocation_pool_df, target_results_df, allocation_method, allocation_policy, ucb_scale, epsilon, greed_factor, allocation_strategy, allocation_order, num_targets, simulation_rounds):\n",
    "    # Set up defaults - nothing picked yet\n",
    "    allocation_pool_df[picked_col] = 0\n",
    "    allocation_pool_df[pool_round_col] = 0\n",
    "    allocation_pool_df[reward_col] = 0\n",
    "    allocation_pool_df[regret_col] = 0\n",
    "\n",
    "    timesteps = {}\n",
    "\n",
    "    for allocation_round in range(1, simulation_rounds+1):\n",
    "\n",
    "        remaining_allocation_pool_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col]==0])\n",
    "\n",
    "        # set up an allocator\n",
    "        # TODO: I think this should be in the sequence of:\n",
    "        #  allocation_pool_df, arm_name_col, target_id_col, confidence_rank_col, allocation_method, strategy, order\n",
    "        # TODO: Would be good to explain that Allocator introduces a picked column to the dataframe. This is masked behavior.\n",
    "        #       I may rely on this column if I know about it, but not sure if it will ever go away?\n",
    "        allocator = Allocator(allocation_method, num_targets, remaining_allocation_pool_df, arm_name_col,\n",
    "                              confidence_rank_col, target_id_col, strategy=allocation_strategy,\n",
    "                              order=allocation_order) # another idea is to provide a dictionary mapping to simplify the calling interface\n",
    "\n",
    "        # retreive targets\n",
    "        targets = allocator.allocate_pool() # TODO: can we change this to allocator.retrieve_targets(num_targets) ? Also, might be nice to have a DF return option\n",
    "\n",
    "        # convert to dataframe\n",
    "        targets_df = pd.DataFrame(targets, columns=[target_id_col, arm_name_col])\n",
    "        targets_df[pool_round_col] = allocation_round\n",
    "        targets_df = targets_df[[target_id_col, pool_round_col]]\n",
    "\n",
    "        # join targets with universe of allocations\n",
    "        targets_df = targets_df.set_index([target_id_col])\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col])\n",
    "        allocation_pool_df.update(targets_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()   \n",
    "\n",
    "        # add indicator for the arm we picked\n",
    "        target_arm_picked_df = pd.DataFrame(targets, columns=[target_id_col, arm_name_col])\n",
    "        target_arm_picked_df[picked_col] = 1\n",
    "        target_arm_picked_df = target_arm_picked_df[[target_id_col, arm_name_col, picked_col]]\n",
    "\n",
    "        # join target model we picked with the universe of allocations\n",
    "        target_arm_picked_df = target_arm_picked_df.set_index([target_id_col, arm_name_col])\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col, arm_name_col])\n",
    "        allocation_pool_df.update(target_arm_picked_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()\n",
    "\n",
    "        # evaluate this allocation pool with the bandit\n",
    "        last_allocation_pool_df = allocation_pool_df[allocation_pool_df[pool_round_col] == allocation_round]\n",
    "\n",
    "        # sync up results\n",
    "        last_allocation_pool_df = last_allocation_pool_df.set_index([target_id_col])\n",
    "        last_allocation_pool_df.update(target_results_df)\n",
    "        last_allocation_pool_df = last_allocation_pool_df.reset_index(level=0)\n",
    "\n",
    "        # update results in the master pool set\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col, arm_name_col, confidence_rank_col, pool_round_col, reward_col, regret_col, picked_col])\n",
    "        last_allocation_pool_df = last_allocation_pool_df.set_index([target_id_col, arm_name_col, confidence_rank_col, pool_round_col, reward_col, regret_col, picked_col])\n",
    "        allocation_pool_df.update(last_allocation_pool_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()\n",
    "\n",
    "        TP = allocation_pool_df[result_col] * allocation_pool_df[confidence_rank_col] * allocation_pool_df[picked_col]\n",
    "        TN = (1 - allocation_pool_df[result_col]) * (1 - allocation_pool_df[confidence_rank_col]) * (1 - allocation_pool_df[picked_col])\n",
    "        FN = allocation_pool_df[result_col] * (1 - allocation_pool_df[confidence_rank_col]) * (1 - allocation_pool_df[picked_col])\n",
    "        FP = (1 - allocation_pool_df[result_col]) * allocation_pool_df[confidence_rank_col] * allocation_pool_df[picked_col]\n",
    "\n",
    "        # update rewards from the results we synced\n",
    "        allocation_pool_df[reward_col] = TP\n",
    "\n",
    "        # update regrets from the results we synced\n",
    "        # regrets can be false positives or false negatives (type 1 and type 2 error)\n",
    "        allocation_pool_df[regret_col] = FP\n",
    "\n",
    "        # prepare a dataframe with our results from this round and all prior rounds for the bandit to evaluate\n",
    "        results_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col] != 0])\n",
    "\n",
    "        if results_df[(results_df[result_col]==1)&(results_df[reward_col]==1)].shape[0] == 0:\n",
    "            # no more results left after a steady state - reset selections to include possibilities from the other models\n",
    "\n",
    "            TP = allocation_pool_df[result_col] * allocation_pool_df[confidence_rank_col]\n",
    "            TN = (1 - allocation_pool_df[result_col]) * (1 - allocation_pool_df[confidence_rank_col])\n",
    "            FN = allocation_pool_df[result_col] * (1 - allocation_pool_df[confidence_rank_col])\n",
    "            FP = (1 - allocation_pool_df[result_col]) * allocation_pool_df[confidence_rank_col]\n",
    "\n",
    "            # update rewards from the results we synced\n",
    "            allocation_pool_df[reward_col] = TP\n",
    "\n",
    "            # update regrets from the results we synced\n",
    "            # regrets can be false positives or false negatives (type 1 and type 2 error)\n",
    "            allocation_pool_df[regret_col] = FN\n",
    "\n",
    "            # prepare a dataframe with just our results of this round for the bandit to evaluate\n",
    "            results_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col] == allocation_round])\n",
    "\n",
    "\n",
    "        # prepare dictionary of results for timestep tracking\n",
    "        stats = results_df[[arm_name_col, reward_col, regret_col]].groupby(arm_name_col).agg({reward_col:['count','sum'],\n",
    "                                                                                              regret_col:['sum']})\n",
    "        stats.columns = stats.columns.to_flat_index()\n",
    "        stats.columns = ['_'.join(tup).rstrip('_') for tup in stats.columns.values]\n",
    "        stats = pd.DataFrame(stats).reset_index()\n",
    "        stats.rename(columns={'target_reward_count': 'reward_evaluation_size', 'target_reward_sum': 'rewards', 'target_regret_sum':'regrets'}, inplace=True)\n",
    "        stats = stats.set_index([arm_name_col])\n",
    "        stats['allocation'] = 0.0\n",
    "        allocs = pd.DataFrame.from_dict(allocation_method, orient='index', columns = ['allocation'])\n",
    "        allocs.index.name=arm_name_col\n",
    "        stats.update(allocs)\n",
    "        stats = stats.reset_index()\n",
    "\n",
    "        timesteps[allocation_round] = stats.to_dict(orient='records')\n",
    "\n",
    "        # set up a multi-arm bandit and calculate allocations to each arm.\n",
    "        bandit = Bandit(results_df, arm_name_col, reward_col, regret_col, policy = allocation_policy, t = allocation_round, ucb_scale = ucb_scale, epsilon = epsilon, greed_factor = greed_factor)\n",
    "\n",
    "        # use these allocations for the next round\n",
    "        allocation_method = bandit.get_new_allocations()# bandit.make_allocs().set_index(arm_name_col)['allocation'].to_dict()\n",
    "        \n",
    "    print(\"Allocations after round\", allocation_round, \":\\n\", allocation_method)\n",
    "    #display(bandit.get_allocation_stats())\n",
    "    \n",
    "    return timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('perm_idx', 1), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.237623775645302, 'model5_lgbm': 0.17852678602966668, 'model2_lr': 0.16799011346253792, 'model1_rf': 0.15730073549588555, 'model_baseline': 0.13225704997401422, 'model4_xgb': 0.12630153939259361}\n",
      "(('perm_idx', 2), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.4540191965701357, 'model2_lr': 0.1786026719384297, 'model5_lgbm': 0.1283215032877796, 'model1_rf': 0.09000677053335682, 'model4_xgb': 0.0774178584300948, 'model_baseline': 0.07163199924020333}\n",
      "(('perm_idx', 3), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9891237408239338, 'model2_lr': 0.0028281661167956053, 'model_baseline': 0.0024444335969012717, 'model4_xgb': 0.0022170742185889276, 'model5_lgbm': 0.0018055360570459794, 'model1_rf': 0.0015810491867343278}\n",
      "(('perm_idx', 4), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.35583822487025374, 'model5_lgbm': 0.1576319635923527, 'model1_rf': 0.1535723172770222, 'model2_lr': 0.13637852111797535, 'model_baseline': 0.10199092879988171, 'model3_lr': 0.0945880443425143}\n",
      "(('perm_idx', 5), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.8079059855108882, 'model5_lgbm': 0.07198195280988248, 'model1_rf': 0.03317637120754073, 'model2_lr': 0.03275325776749773, 'model3_lr': 0.027553584796271152, 'model_baseline': 0.02662884790791981}\n",
      "(('perm_idx', 6), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9991136365175951, 'model5_lgbm': 0.0002525338850242876, 'model1_rf': 0.00017562217834411861, 'model2_lr': 0.00016027027550415625, 'model_baseline': 0.00015605457194271536, 'model3_lr': 0.00014188257158956541}\n",
      "(('perm_idx', 7), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.25713080152254714, 'model5_lgbm': 0.19163018742436533, 'model1_rf': 0.18140048778665394, 'model2_lr': 0.13269490294441616, 'model_baseline': 0.11986960787624072, 'model4_xgb': 0.11727401244577662}\n",
      "(('perm_idx', 8), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.4670344440185371, 'model1_rf': 0.18767759199368755, 'model5_lgbm': 0.17125147602106372, 'model4_xgb': 0.06750948572000921, 'model_baseline': 0.060043065314717266, 'model2_lr': 0.04648393693198522}\n",
      "(('perm_idx', 9), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', 3), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9907946703799833, 'model1_rf': 0.0021743868812528757, 'model5_lgbm': 0.0021743868812528757, 'model4_xgb': 0.0020562799533618403, 'model_baseline': 0.0014180804480501716, 'model2_lr': 0.0013821954560989903}\n",
      "(('perm_idx', 10), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.30224413667973105, 'model2_lr': 0.18380713949028485, 'model5_lgbm': 0.17381974009051668, 'model1_rf': 0.13956033781471142, 'model4_xgb': 0.10391245992911848, 'model_baseline': 0.0966561859956375}\n",
      "(('perm_idx', 11), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6557438534191761, 'model2_lr': 0.21706129104232272, 'model5_lgbm': 0.045627979228398306, 'model_baseline': 0.04259621165430607, 'model4_xgb': 0.024456931635625512, 'model1_rf': 0.01451373302017132}\n",
      "(('perm_idx', 12), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9997761871630396, 'model2_lr': 7.762098771462154e-05, 'model_baseline': 7.762098771462148e-05, 'model4_xgb': 4.044607552121534e-05, 'model5_lgbm': 1.9795999867551252e-05, 'model1_rf': 8.328786142245804e-06}\n",
      "(('perm_idx', 13), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.28811532843760207, 'model2_lr': 0.18743712076622304, 'model5_lgbm': 0.17397566053363944, 'model1_rf': 0.13950074245155467, 'model4_xgb': 0.11367028747749057, 'model_baseline': 0.09730086033349028}\n",
      "(('perm_idx', 14), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6577865120269586, 'model2_lr': 0.13987371069615012, 'model_baseline': 0.08656979378349064, 'model5_lgbm': 0.05337962893814955, 'model4_xgb': 0.0326648580833912, 'model1_rf': 0.029725496471859958}\n",
      "(('perm_idx', 15), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9997343652754783, 'model2_lr': 8.745097789859343e-05, 'model_baseline': 6.863028015203866e-05, 'model4_xgb': 6.577519823536619e-05, 'model5_lgbm': 3.0003383537514394e-05, 'model1_rf': 1.377488469810369e-05}\n",
      "(('perm_idx', 16), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.2777035059403163, 'model5_lgbm': 0.17205008234257346, 'model2_lr': 0.16325386759570729, 'model1_rf': 0.16307421548537884, 'model4_xgb': 0.11650311622652054, 'model_baseline': 0.10741521240950358}\n",
      "(('perm_idx', 17), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.5834016717952001, 'model2_lr': 0.21089515138331835, 'model5_lgbm': 0.08110737139973065, 'model1_rf': 0.048988514215820615, 'model_baseline': 0.04322493334250438, 'model4_xgb': 0.03238235786342583}\n",
      "(('perm_idx', 18), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9995963366067934, 'model2_lr': 0.00013945495769755, 'model_baseline': 0.0001008507587136951, 'model4_xgb': 0.00010085075871369503, 'model5_lgbm': 4.11682982485005e-05, 'model1_rf': 2.133861983324565e-05}\n",
      "(('perm_idx', 19), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.5628829720790247, 'model5_lgbm': 0.16077619218034744, 'model1_rf': 0.13301705924867618, 'model_baseline': 0.05495265540724349, 'model2_lr': 0.053727030911805036, 'model3_lr': 0.03464409017290322}\n",
      "(('perm_idx', 20), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9893785265407584, 'model5_lgbm': 0.004600437795318373, 'model_baseline': 0.002840531305212292, 'model3_lr': 0.0013739320340842582, 'model2_lr': 0.0012789726342873104, 'model1_rf': 0.0005275996903394455}\n",
      "(('perm_idx', 21), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999660054536982, 'model5_lgbm': 3.269238874474466e-05, 'model3_lr': 6.862416030768916e-07, 'model2_lr': 3.625212647513357e-07, 'model1_rf': 1.59111983117609e-07, 'model_baseline': 9.428270615017114e-08}\n",
      "(('perm_idx', 22), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.6374194736732179, 'model1_rf': 0.12002407002555329, 'model5_lgbm': 0.09457478225804766, 'model2_lr': 0.07856919732547211, 'model_baseline': 0.046484942624485735, 'model3_lr': 0.022927534093223224}\n",
      "(('perm_idx', 23), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9703715701354244, 'model5_lgbm': 0.012521836752141729, 'model1_rf': 0.007750237575855686, 'model2_lr': 0.0047241950979043995, 'model_baseline': 0.0029752578896372547, 'model3_lr': 0.001656902549036281}\n",
      "(('perm_idx', 24), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999976502908032, 'model2_lr': 9.036611550637586e-07, 'model5_lgbm': 4.973996284147219e-07, 'model1_rf': 4.4400668629568757e-07, 'model_baseline': 3.059880730194648e-07, 'model3_lr': 1.986536540352055e-07}\n",
      "(('perm_idx', 25), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.522773040993525, 'model5_lgbm': 0.21969848836179237, 'model1_rf': 0.11798054097423251, 'model_baseline': 0.06453602618883612, 'model2_lr': 0.059750368360558706, 'model3_lr': 0.015261535121055332}\n",
      "(('perm_idx', 26), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.982591051407161, 'model2_lr': 0.00901650893636888, 'model1_rf': 0.003153780262408569, 'model_baseline': 0.0026159605944839777, 'model5_lgbm': 0.0018024765193764133, 'model3_lr': 0.0008202222802011038}\n",
      "(('perm_idx', 27), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999868386672494, 'model5_lgbm': 1.0472596170564153e-05, 'model2_lr': 1.2479187870032124e-06, 'model1_rf': 9.457354521677776e-07, 'model_baseline': 4.2883623641553994e-07, 'model3_lr': 6.624610452080599e-08}\n",
      "(('perm_idx', 28), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.3484634167062178, 'model1_rf': 0.2117592474008226, 'model5_lgbm': 0.20818327883763335, 'model4_xgb': 0.08956409944644979, 'model_baseline': 0.08120462800403787, 'model2_lr': 0.06082532960483856}\n",
      "(('perm_idx', 29), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6937692728579479, 'model1_rf': 0.15266065457804584, 'model5_lgbm': 0.10354805067788819, 'model4_xgb': 0.032814663837657736, 'model_baseline': 0.01110404276453679, 'model2_lr': 0.006103315283923445}\n",
      "(('perm_idx', 30), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', 3), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9999003419480211, 'model1_rf': 4.185388631650222e-05, 'model4_xgb': 2.774673863416687e-05, 'model5_lgbm': 2.6242579666458866e-05, 'model_baseline': 2.6850283678171033e-06, 'model2_lr': 1.129818994021255e-06}\n",
      "(('perm_idx', 31), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.3284192313341858, 'model5_lgbm': 0.2079118984836885, 'model1_rf': 0.2033554356987397, 'model_baseline': 0.09085255854972363, 'model4_xgb': 0.08584450148834451, 'model2_lr': 0.08361637444531798}\n",
      "(('perm_idx', 32), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6263302307229658, 'model1_rf': 0.18240727287156852, 'model5_lgbm': 0.13514387307345802, 'model4_xgb': 0.03244976011540041, 'model_baseline': 0.01755904504984093, 'model2_lr': 0.006109818166766373}\n",
      "(('perm_idx', 33), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', 3), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9998498896799636, 'model1_rf': 5.453095901379305e-05, 'model4_xgb': 4.5162969713662036e-05, 'model5_lgbm': 4.299625536074969e-05, 'model_baseline': 4.594012152323152e-06, 'model2_lr': 2.8261237958652255e-06}\n",
      "(('perm_idx', 34), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.31140869315612285, 'model5_lgbm': 0.20282378127068545, 'model1_rf': 0.1874363366433287, 'model2_lr': 0.11859844014836088, 'model_baseline': 0.09545804829610219, 'model4_xgb': 0.08427470048540002}\n",
      "(('perm_idx', 35), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 1.5), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.5793981952868471, 'model1_rf': 0.2052341898639487, 'model5_lgbm': 0.15709305846332094, 'model4_xgb': 0.031316545477464576, 'model_baseline': 0.019136878373950318, 'model2_lr': 0.007821132534468204}\n",
      "(('perm_idx', 36), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', 3), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9997795564801796, 'model1_rf': 8.500206346102946e-05, 'model5_lgbm': 6.592262556203881e-05, 'model4_xgb': 5.4865012834255594e-05, 'model_baseline': 8.793318024038861e-06, 'model2_lr': 5.860499938880978e-06}\n",
      "(('perm_idx', 37), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model4_xgb': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 38), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9837064118022794, 'model1_rf': 0.003258717639544145, 'model2_lr': 0.003258717639544145, 'model4_xgb': 0.003258717639544145, 'model5_lgbm': 0.003258717639544145, 'model_baseline': 0.003258717639544145}\n",
      "(('perm_idx', 39), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9999451333260178, 'model1_rf': 1.097333479644464e-05, 'model2_lr': 1.097333479644464e-05, 'model4_xgb': 1.097333479644464e-05, 'model5_lgbm': 1.097333479644464e-05, 'model_baseline': 1.097333479644464e-05}\n",
      "(('perm_idx', 40), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6500000000000001, 'model1_rf': 0.07, 'model2_lr': 0.07, 'model4_xgb': 0.07, 'model5_lgbm': 0.07, 'model_baseline': 0.07}\n",
      "(('perm_idx', 41), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.8498312028095432, 'model1_rf': 0.03003375943809133, 'model2_lr': 0.03003375943809133, 'model4_xgb': 0.03003375943809133, 'model5_lgbm': 0.03003375943809133, 'model_baseline': 0.03003375943809133}\n",
      "(('perm_idx', 42), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9937938771079107, 'model1_rf': 0.0012412245784178906, 'model2_lr': 0.0012412245784178906, 'model4_xgb': 0.0012412245784178906, 'model5_lgbm': 0.0012412245784178906, 'model_baseline': 0.0012412245784178906}\n",
      "(('perm_idx', 43), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.45, 'model1_rf': 0.11000000000000003, 'model2_lr': 0.11000000000000003, 'model4_xgb': 0.11000000000000003, 'model5_lgbm': 0.11000000000000003, 'model_baseline': 0.11000000000000003}\n",
      "(('perm_idx', 44), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6233314819933395, 'model1_rf': 0.07533370360133207, 'model2_lr': 0.07533370360133207, 'model4_xgb': 0.07533370360133207, 'model5_lgbm': 0.07533370360133207, 'model_baseline': 0.07533370360133207}\n",
      "(('perm_idx', 45), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9319390468398446, 'model1_rf': 0.0136121906320311, 'model2_lr': 0.0136121906320311, 'model4_xgb': 0.0136121906320311, 'model5_lgbm': 0.0136121906320311, 'model_baseline': 0.0136121906320311}\n",
      "(('perm_idx', 46), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model4_xgb': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 47), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9837064118022794, 'model1_rf': 0.003258717639544145, 'model2_lr': 0.003258717639544145, 'model3_lr': 0.003258717639544145, 'model5_lgbm': 0.003258717639544145, 'model_baseline': 0.003258717639544145}\n",
      "(('perm_idx', 48), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999451333260178, 'model1_rf': 1.097333479644464e-05, 'model2_lr': 1.097333479644464e-05, 'model3_lr': 1.097333479644464e-05, 'model5_lgbm': 1.097333479644464e-05, 'model_baseline': 1.097333479644464e-05}\n",
      "(('perm_idx', 49), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.6500000000000001, 'model1_rf': 0.07, 'model2_lr': 0.07, 'model3_lr': 0.07, 'model5_lgbm': 0.07, 'model_baseline': 0.07}\n",
      "(('perm_idx', 50), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.8498312028095434, 'model1_rf': 0.030033759438091334, 'model2_lr': 0.030033759438091334, 'model3_lr': 0.030033759438091334, 'model5_lgbm': 0.030033759438091334, 'model_baseline': 0.030033759438091334}\n",
      "(('perm_idx', 51), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9937938771079107, 'model1_rf': 0.0012412245784178906, 'model2_lr': 0.0012412245784178906, 'model3_lr': 0.0012412245784178906, 'model5_lgbm': 0.0012412245784178906, 'model_baseline': 0.0012412245784178906}\n",
      "(('perm_idx', 52), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.44999999999999996, 'model1_rf': 0.11000000000000001, 'model2_lr': 0.11000000000000001, 'model3_lr': 0.11000000000000001, 'model5_lgbm': 0.11000000000000001, 'model_baseline': 0.11000000000000001}\n",
      "(('perm_idx', 53), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 1.5), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.6233314819933395, 'model1_rf': 0.07533370360133207, 'model2_lr': 0.07533370360133207, 'model3_lr': 0.07533370360133207, 'model5_lgbm': 0.07533370360133207, 'model_baseline': 0.07533370360133207}\n",
      "(('perm_idx', 54), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 3), ('strategy', 'round-robin'), ('order', 'random'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9319390468398446, 'model1_rf': 0.0136121906320311, 'model2_lr': 0.0136121906320311, 'model3_lr': 0.0136121906320311, 'model5_lgbm': 0.0136121906320311, 'model_baseline': 0.0136121906320311}\n",
      "(('perm_idx', 55), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model4_xgb': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 56), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1.5), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9837064118022794, 'model1_rf': 0.003258717639544145, 'model2_lr': 0.003258717639544145, 'model4_xgb': 0.003258717639544145, 'model5_lgbm': 0.003258717639544145, 'model_baseline': 0.003258717639544145}\n",
      "(('perm_idx', 57), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 3), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9999451333260178, 'model1_rf': 1.097333479644464e-05, 'model2_lr': 1.097333479644464e-05, 'model4_xgb': 1.097333479644464e-05, 'model5_lgbm': 1.097333479644464e-05, 'model_baseline': 1.097333479644464e-05}\n",
      "(('perm_idx', 58), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6500000000000001, 'model1_rf': 0.07, 'model2_lr': 0.07, 'model4_xgb': 0.07, 'model5_lgbm': 0.07, 'model_baseline': 0.07}\n",
      "(('perm_idx', 59), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 1.5), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.8498312028095432, 'model1_rf': 0.03003375943809133, 'model2_lr': 0.03003375943809133, 'model4_xgb': 0.03003375943809133, 'model5_lgbm': 0.03003375943809133, 'model_baseline': 0.03003375943809133}\n",
      "(('perm_idx', 60), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.35), ('greed_factor', 3), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9937938771079107, 'model1_rf': 0.0012412245784178906, 'model2_lr': 0.0012412245784178906, 'model4_xgb': 0.0012412245784178906, 'model5_lgbm': 0.0012412245784178906, 'model_baseline': 0.0012412245784178906}\n",
      "(('perm_idx', 61), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.45, 'model1_rf': 0.11000000000000003, 'model2_lr': 0.11000000000000003, 'model4_xgb': 0.11000000000000003, 'model5_lgbm': 0.11000000000000003, 'model_baseline': 0.11000000000000003}\n",
      "(('perm_idx', 62), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 1.5), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.6233314819933395, 'model1_rf': 0.07533370360133207, 'model2_lr': 0.07533370360133207, 'model4_xgb': 0.07533370360133207, 'model5_lgbm': 0.07533370360133207, 'model_baseline': 0.07533370360133207}\n",
      "(('perm_idx', 63), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.55), ('greed_factor', 3), ('strategy', 'greedy'), ('order', 'best'), ('pool_size', 250), ('num_rounds', 50))\n",
      "Allocations after round 50 :\n",
      " {'model3_lr': 0.9319390468398446, 'model1_rf': 0.0136121906320311, 'model2_lr': 0.0136121906320311, 'model4_xgb': 0.0136121906320311, 'model5_lgbm': 0.0136121906320311, 'model_baseline': 0.0136121906320311}\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "simulations = {}\n",
    "for _, v in permutations.items():\n",
    "    policy = v['policy']\n",
    "    ucb_scale = None\n",
    "    epsilon = None\n",
    "    greed_factor = None\n",
    "    if policy == 'UCB1':\n",
    "        for strategy, order in v['allocations']:\n",
    "            for greed_factor in v['options']['greed_factor']:\n",
    "                perm_idx = make_perm_idx(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                print(perm_idx)\n",
    "                timesteps = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                               ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                payload = make_perm_dict(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                payload['timesteps'] = timesteps\n",
    "                simulations[perm_idx] = payload\n",
    "                i = i + 1\n",
    "    if policy == 'Bayes_UCB':\n",
    "        for strategy, order in v['allocations']:\n",
    "            for ucb_scale in v['options']['ucb_scale']:\n",
    "                for greed_factor in v['options']['greed_factor']:\n",
    "                    perm_idx = make_perm_idx(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    print(perm_idx)\n",
    "                    timesteps = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                                   ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    payload = make_perm_dict(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    payload['timesteps'] = timesteps\n",
    "                    simulations[perm_idx] = payload\n",
    "                    i = i + 1\n",
    "    if policy == 'epsilon_greedy':\n",
    "        for strategy, order in v['allocations']:\n",
    "            for epsilon in v['options']['epsilon']:\n",
    "                for greed_factor in v['options']['greed_factor']:\n",
    "                    perm_idx = make_perm_idx(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    print(perm_idx)\n",
    "                    timesteps = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                                   ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    payload = make_perm_dict(i, policy, ucb_scale, epsilon, greed_factor, strategy, order, num_targets, simulation_rounds)\n",
    "                    payload['timesteps'] = timesteps\n",
    "                    simulations[perm_idx] = payload\n",
    "                    i = i + 1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_to_json(data):\n",
    "    if data is None or isinstance(data, (bool, int, str)):\n",
    "        return data\n",
    "    if isinstance(data, (tuple, frozenset)):\n",
    "        return str(data)\n",
    "    raise TypeError\n",
    "\n",
    "def to_json(data):\n",
    "    if data is None or isinstance(data, (bool, int, tuple, range, str, list)):\n",
    "        return data\n",
    "    if isinstance(data, (set, frozenset)):\n",
    "        return sorted(data)\n",
    "    if isinstance(data, dict):\n",
    "        return {key_to_json(key): to_json(data[key]) for key in data}\n",
    "    raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write simulation sequence to a json file\n",
    "with open('simulations.json', 'w') as json_file:\n",
    "    json.dump(simulations, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:min_ds] *",
   "language": "python",
   "name": "conda-env-min_ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
