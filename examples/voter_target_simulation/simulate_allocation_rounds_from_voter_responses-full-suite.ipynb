{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from darts.allocation import Allocator\n",
    "from darts.bandit import Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed in 0.028928518295288086 seconds\n"
     ]
    }
   ],
   "source": [
    "# load in the data for the target pool\n",
    "start = time.time()\n",
    "file_loc = 'data/'\n",
    "allocation_pool_df = pd.read_csv(file_loc + 'target_voter_universe.csv')\n",
    "print(f\"... completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>probability</th>\n",
       "      <th>target_round</th>\n",
       "      <th>target_result</th>\n",
       "      <th>target_reward</th>\n",
       "      <th>target_regret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>model1_rf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>model2_lr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>model3_lr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>model4_xgb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>model5_lgbm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>model_baseline</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>model1_rf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>model2_lr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>model3_lr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>model4_xgb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_id        model_id  probability  target_round  target_result  \\\n",
       "0          0       model1_rf          0.0             0              0   \n",
       "1          0       model2_lr          1.0             0              0   \n",
       "2          0       model3_lr          0.0             0              0   \n",
       "3          0      model4_xgb          1.0             0              0   \n",
       "4          0     model5_lgbm          0.0             0              0   \n",
       "5          0  model_baseline          1.0             0              0   \n",
       "6          1       model1_rf          0.0             0              0   \n",
       "7          1       model2_lr          0.0             0              0   \n",
       "8          1       model3_lr          0.0             0              0   \n",
       "9          1      model4_xgb          0.0             0              0   \n",
       "\n",
       "   target_reward  target_regret  \n",
       "0              0              0  \n",
       "1              0              0  \n",
       "2              0              0  \n",
       "3              0              0  \n",
       "4              0              0  \n",
       "5              0              0  \n",
       "6              0              0  \n",
       "7              0              0  \n",
       "8              0              0  \n",
       "9              0              0  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocation_pool_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.54"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(allocation_pool_df.shape[0]/6)/250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed in 0.007098674774169922 seconds\n"
     ]
    }
   ],
   "source": [
    "# load in our target results\n",
    "start = time.time()\n",
    "file_loc = 'data/'\n",
    "target_results_df = pd.read_csv(file_loc + 'voter_responses.csv')\n",
    "print(f\"... completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_id  target_result\n",
       "0          0              1\n",
       "1          1              0\n",
       "2          3              0\n",
       "3          4              0\n",
       "4         16              1\n",
       "5         22              0\n",
       "6         26              0\n",
       "7         27              1\n",
       "8         29              0\n",
       "9         31              0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column that indicates the id of an individual target\n",
    "target_id_col = 'target_id'\n",
    "\n",
    "# Specify the column that provides the confidence ranking (e.g. probability)\n",
    "# that any of the models have on this target resulting in a 'reward'\n",
    "# (e.g. the target outcome of the model)\n",
    "confidence_rank_col = 'probability'\n",
    "\n",
    "# Specify the column containing the names of the different 'arms' or\n",
    "# models we're using in the target pool\n",
    "arm_name_col = 'model_id'\n",
    "\n",
    "# Specify column to track the pool round\n",
    "pool_round_col = 'target_round'\n",
    "\n",
    "# Specify the column that indicates the 'result'\n",
    "result_col = 'target_result'\n",
    "\n",
    "# Specify the column that indicates the 'reward'\n",
    "reward_col = 'target_reward'\n",
    "\n",
    "# Specify the column that indicates the 'regret'\n",
    "regret_col = 'target_regret'\n",
    "\n",
    "# Specify the column that indicates which arm was 'picked'\n",
    "picked_col = 'model_picked'\n",
    "\n",
    "# Specify an initial allocation method across the pool of models. These must sum to 1.\n",
    "# For the simulation, we'll start with an equal allocation for each model.\n",
    "allocation_method = {\n",
    "    'model_baseline': (1/6),\n",
    "    'model1_rf': (1/6),\n",
    "    'model2_lr': (1/6),\n",
    "    'model3_lr': (1/6),\n",
    "    'model4_xgb': (1/6),\n",
    "    'model5_lgbm': (1/6)\n",
    "}\n",
    "\n",
    "# Specify number of rounds to simulate\n",
    "simulation_rounds = 50\n",
    "\n",
    "# Specify the number of targets to pull in the first round.\n",
    "num_targets = 250\n",
    "\n",
    "# Specify the allocation policy\n",
    "allocation_policy = 'UCB1'\n",
    "\n",
    "# Specify the allocation strategy\n",
    "allocation_strategy = 'round-robin'\n",
    "\n",
    "# Specify the allocation order\n",
    "allocation_order = 'best'\n",
    "\n",
    "# bandit parameters\n",
    "ucb_scale = 1.96\n",
    "epsilon = 0.1\n",
    "greed_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the file of responses for updating of results\n",
    "target_results_df = target_results_df.set_index([target_id_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = {\n",
    "    1: {'policy': 'UCB1',\n",
    "        'options': {},\n",
    "        'allocations': [('round-robin', 'best'), ('round-robin', 'random'), ('greedy', 'best')]\n",
    "       },\n",
    "    2: {'policy': 'Bayes_UCB',\n",
    "        'options': {'ucb_scale': [1, 2, 3]},\n",
    "        'allocations': [('round-robin', 'best'), ('round-robin', 'random'), ('greedy', 'best')]\n",
    "       },\n",
    "    3: {'policy': 'epsilon_greedy',\n",
    "        'options': {'episilon': [0.05, 0.10, 0.25],\n",
    "                    'greed_factor': [1, 10, 100]},\n",
    "        'allocations': [('round-robin', 'best'), ('round-robin', 'random'), ('greedy', 'best')]\n",
    "       }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_perm_idx(idx, policy, ucb_scale, epsilon, greed_factor, strategy, order):\n",
    "    return tuple({\n",
    "        'perm_idx': idx,\n",
    "        'policy': policy,\n",
    "        'ucb_scale': ucb_scale,\n",
    "        'epsilon': epsilon,\n",
    "        'greed_factor': greed_factor,\n",
    "        'strategy': strategy,\n",
    "        'order': order\n",
    "    }.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_simulation(allocation_pool_df, target_results_df, allocation_method, allocation_policy, ucb_scale, epsilon, greed_factor, allocation_strategy, allocation_order):\n",
    "    # Set up defaults - nothing picked yet\n",
    "    allocation_pool_df[picked_col] = 0\n",
    "    allocation_pool_df[pool_round_col] = 0\n",
    "    allocation_pool_df[reward_col] = 0\n",
    "    allocation_pool_df[regret_col] = 0\n",
    "    target_reward_cummulative_sum = 0\n",
    "    target_regret_cummulative_sum = 0\n",
    "\n",
    "    timesteps = {}\n",
    "\n",
    "    for allocation_round in range(1, simulation_rounds+1):\n",
    "\n",
    "        remaining_allocation_pool_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col]==0])\n",
    "\n",
    "        # set up an allocator\n",
    "        # TODO: I think this should be in the sequence of:\n",
    "        #  allocation_pool_df, arm_name_col, target_id_col, confidence_rank_col, allocation_method, strategy, order\n",
    "        # TODO: Would be good to explain that Allocator introduces a picked column to the dataframe. This is masked behavior.\n",
    "        #       I may rely on this column if I know about it, but not sure if it will ever go away?\n",
    "        allocator = Allocator(allocation_method, num_targets, remaining_allocation_pool_df, arm_name_col,\n",
    "                              confidence_rank_col, target_id_col, strategy=allocation_strategy,\n",
    "                              order=allocation_order) # another idea is to provide a dictionary mapping to simplify the calling interface\n",
    "\n",
    "        # retreive targets\n",
    "        targets = allocator.allocate_pool() # TODO: can we change this to allocator.retrieve_targets(num_targets) ? Also, might be nice to have a DF return option\n",
    "\n",
    "        # convert to dataframe\n",
    "        targets_df = pd.DataFrame(targets, columns=[target_id_col, arm_name_col])\n",
    "        targets_df[pool_round_col] = allocation_round\n",
    "        targets_df = targets_df[[target_id_col, pool_round_col]]\n",
    "\n",
    "        # join targets with universe of allocations\n",
    "        targets_df = targets_df.set_index([target_id_col])\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col])\n",
    "        allocation_pool_df.update(targets_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()   \n",
    "\n",
    "        # add indicator for the arm we picked\n",
    "        target_arm_picked_df = pd.DataFrame(targets, columns=[target_id_col, arm_name_col])\n",
    "        target_arm_picked_df[picked_col] = 1\n",
    "        target_arm_picked_df = target_arm_picked_df[[target_id_col, arm_name_col, picked_col]]\n",
    "\n",
    "        # join target model we picked with the universe of allocations\n",
    "        target_arm_picked_df = target_arm_picked_df.set_index([target_id_col, arm_name_col])\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col, arm_name_col])\n",
    "        allocation_pool_df.update(target_arm_picked_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()\n",
    "\n",
    "        # evaluate this allocation pool with the bandit\n",
    "        last_allocation_pool_df = allocation_pool_df[allocation_pool_df[pool_round_col] == allocation_round]\n",
    "\n",
    "        # sync up results\n",
    "        last_allocation_pool_df = last_allocation_pool_df.set_index([target_id_col])\n",
    "        last_allocation_pool_df.update(target_results_df)\n",
    "        last_allocation_pool_df = last_allocation_pool_df.reset_index(level=0)\n",
    "\n",
    "        # update results in the master pool set\n",
    "        allocation_pool_df = allocation_pool_df.set_index([target_id_col, arm_name_col, confidence_rank_col, pool_round_col, reward_col, regret_col, picked_col])\n",
    "        last_allocation_pool_df = last_allocation_pool_df.set_index([target_id_col, arm_name_col, confidence_rank_col, pool_round_col, reward_col, regret_col, picked_col])\n",
    "        allocation_pool_df.update(last_allocation_pool_df)\n",
    "        allocation_pool_df = allocation_pool_df.reset_index()\n",
    "\n",
    "        TP = allocation_pool_df[result_col] * allocation_pool_df[confidence_rank_col] * allocation_pool_df[picked_col]\n",
    "        TN = (1 - allocation_pool_df[result_col]) * (1 - allocation_pool_df[confidence_rank_col]) * (1 - allocation_pool_df[picked_col])\n",
    "        FN = allocation_pool_df[result_col] * (1 - allocation_pool_df[confidence_rank_col]) * (1 - allocation_pool_df[picked_col])\n",
    "        FP = (1 - allocation_pool_df[result_col]) * allocation_pool_df[confidence_rank_col] * allocation_pool_df[picked_col]\n",
    "\n",
    "        # update rewards from the results we synced\n",
    "        allocation_pool_df[reward_col] = TP\n",
    "\n",
    "        # update regrets from the results we synced\n",
    "        # regrets can be false positives or false negatives (type 1 and type 2 error)\n",
    "        allocation_pool_df[regret_col] = FN\n",
    "\n",
    "        # prepare a dataframe with just our results of this round for the bandit to evaluate\n",
    "        results_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col] == allocation_round])\n",
    "\n",
    "        if results_df[(results_df[result_col]==1)&(results_df[reward_col]==1)].shape[0] == 0:\n",
    "            # no more results left after a steady state - reset selections to include possibilities from the other models\n",
    "\n",
    "            TP = allocation_pool_df[result_col] * allocation_pool_df[confidence_rank_col]\n",
    "            TN = (1 - allocation_pool_df[result_col]) * (1 - allocation_pool_df[confidence_rank_col])\n",
    "            FN = allocation_pool_df[result_col] * (1 - allocation_pool_df[confidence_rank_col])\n",
    "            FP = (1 - allocation_pool_df[result_col]) * allocation_pool_df[confidence_rank_col]\n",
    "\n",
    "            # update rewards from the results we synced\n",
    "            allocation_pool_df[reward_col] = TP\n",
    "\n",
    "            # update regrets from the results we synced\n",
    "            # regrets can be false positives or false negatives (type 1 and type 2 error)\n",
    "            allocation_pool_df[regret_col] = FN\n",
    "\n",
    "            # prepare a dataframe with just our results of this round for the bandit to evaluate\n",
    "            results_df = pd.DataFrame.copy(allocation_pool_df[allocation_pool_df[pool_round_col] == allocation_round])\n",
    "\n",
    "\n",
    "        # prepare dictionary of results for timestep tracking\n",
    "        stats = results_df[[arm_name_col, reward_col, regret_col]].groupby(arm_name_col).agg({reward_col:['count','sum'],\n",
    "                                                                                              regret_col:['sum']})\n",
    "        stats.columns = stats.columns.to_flat_index()\n",
    "        stats.columns = ['_'.join(tup).rstrip('_') for tup in stats.columns.values]\n",
    "        stats = pd.DataFrame(stats).reset_index()\n",
    "        stats.rename(columns={'target_reward_count': 'total_pool_size', 'target_reward_sum': 'rewards', 'target_regret_sum':'regrets'}, inplace=True)\n",
    "        stats = stats.set_index([arm_name_col])\n",
    "        stats['allocation'] = 0.0\n",
    "        allocs = pd.DataFrame.from_dict(allocation_method, orient='index', columns = ['allocation'])\n",
    "        allocs.index.name=arm_name_col\n",
    "        stats.update(allocs)\n",
    "        stats = stats.reset_index()\n",
    "        target_reward_cummulative_sum = pd.Series(target_reward_cummulative_sum + stats['rewards'])\n",
    "        stats['cummulative_rewards'] = target_reward_cummulative_sum\n",
    "        target_regret_cummulative_sum = pd.Series(target_regret_cummulative_sum + stats['regrets'])\n",
    "        stats['cummulative_regrets'] = target_regret_cummulative_sum\n",
    "        timesteps[allocation_round] = stats.to_dict(orient='records')\n",
    "\n",
    "        # set up a multi-arm bandit and calculate allocations to each arm.\n",
    "        bandit = Bandit(results_df, arm_name_col, reward_col, regret_col, policy = allocation_policy, t = allocation_round, ucb_scale = ucb_scale, epsilon = epsilon, greed_factor = greed_factor)\n",
    "\n",
    "        # use these allocations for the next round\n",
    "        allocation_method = bandit.get_new_allocations()# bandit.make_allocs().set_index(arm_name_col)['allocation'].to_dict()\n",
    "        \n",
    "    print(\"Allocations after round\", allocation_round, \":\\n\", allocation_method)\n",
    "    #display(bandit.get_allocation_stats())\n",
    "    \n",
    "    return timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('perm_idx', 1), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', None), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model1_rf': 0.17750185935610696, 'model2_lr': 0.17750185935610696, 'model5_lgbm': 0.17027839756314678, 'model3_lr': 0.16305493577018657, 'model4_xgb': 0.15583147397722638, 'model_baseline': 0.15583147397722638}\n",
      "(('perm_idx', 2), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', None), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.1964696828677392, 'model1_rf': 0.16666666666666666, 'model2_lr': 0.1624090929236563, 'model5_lgbm': 0.1624090929236563, 'model_baseline': 0.15815151918064596, 'model3_lr': 0.1538939454376356}\n",
      "(('perm_idx', 3), ('policy', 'UCB1'), ('ucb_scale', None), ('epsilon', None), ('greed_factor', None), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model1_rf': 0.19989420882502837, 'model2_lr': 0.17814527213955525, 'model3_lr': 0.17089562657773089, 'model5_lgbm': 0.16002115823499433, 'model4_xgb': 0.15639633545408216, 'model_baseline': 0.13464739876860907}\n",
      "(('perm_idx', 4), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', None), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.5328148080008955, 'model_baseline': 0.29716508496353705, 'model2_lr': 0.17002010703556744, 'model1_rf': 0.0, 'model3_lr': 0.0, 'model5_lgbm': 0.0}\n",
      "(('perm_idx', 5), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', None), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.6076026858004963, 'model_baseline': 0.3923973141995038, 'model1_rf': 0.0, 'model2_lr': 0.0, 'model3_lr': 0.0, 'model5_lgbm': 0.0}\n",
      "(('perm_idx', 6), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', None), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.36446178939567037, 'model_baseline': 0.20471706222889444, 'model1_rf': 0.1436070494584784, 'model2_lr': 0.1436070494584784, 'model5_lgbm': 0.1436070494584784, 'model3_lr': 0.0}\n",
      "(('perm_idx', 7), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', None), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 1.0, 'model1_rf': 0.0, 'model2_lr': 0.0, 'model3_lr': 0.0, 'model5_lgbm': 0.0, 'model_baseline': 0.0}\n",
      "(('perm_idx', 8), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', None), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9304929484182762, 'model5_lgbm': 0.06950705158172366, 'model1_rf': 0.0, 'model2_lr': 0.0, 'model3_lr': 0.0, 'model_baseline': 0.0}\n",
      "(('perm_idx', 9), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', None), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.6342578864935984, 'model5_lgbm': 0.21527647228986624, 'model1_rf': 0.15046564121653538, 'model2_lr': 0.0, 'model3_lr': 0.0, 'model_baseline': 0.0}\n",
      "(('perm_idx', 10), ('policy', 'Bayes_UCB'), ('ucb_scale', 1), ('epsilon', None), ('greed_factor', None), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.7944660635929659, 'model_baseline': 0.20553393640703407, 'model1_rf': 0.0, 'model2_lr': 0.0, 'model3_lr': 0.0, 'model5_lgbm': 0.0}\n",
      "(('perm_idx', 11), ('policy', 'Bayes_UCB'), ('ucb_scale', 2), ('epsilon', None), ('greed_factor', None), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.6350726022040114, 'model_baseline': 0.36492739779598865, 'model1_rf': 0.0, 'model2_lr': 0.0, 'model3_lr': 0.0, 'model5_lgbm': 0.0}\n",
      "(('perm_idx', 12), ('policy', 'Bayes_UCB'), ('ucb_scale', 3), ('epsilon', None), ('greed_factor', None), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 1.0, 'model1_rf': 0.0, 'model2_lr': 0.0, 'model3_lr': 0.0, 'model5_lgbm': 0.0, 'model_baseline': 0.0}\n",
      "(('perm_idx', 13), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 14), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 10), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model4_xgb': 2.9368033185714593e-17, 'model_baseline': 2.9368033185714593e-17}\n",
      "(('perm_idx', 15), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 100), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model4_xgb': 4.7725295101856255e-166, 'model_baseline': 4.7725295101856255e-166}\n",
      "(('perm_idx', 16), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 17), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 10), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model4_xgb': 2.9368033185714593e-17, 'model_baseline': 2.9368033185714593e-17}\n",
      "(('perm_idx', 18), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 100), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model4_xgb': 4.7725295101856255e-166, 'model_baseline': 4.7725295101856255e-166}\n",
      "(('perm_idx', 19), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 20), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 10), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model4_xgb': 2.9368033185714593e-17, 'model_baseline': 2.9368033185714593e-17}\n",
      "(('perm_idx', 21), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 100), ('strategy', 'round-robin'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model4_xgb': 4.7725295101856255e-166, 'model_baseline': 4.7725295101856255e-166}\n",
      "(('perm_idx', 22), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model1_rf': 0.9, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model4_xgb': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 23), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 10), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model4_xgb': 2.9368033185714593e-17, 'model_baseline': 2.9368033185714593e-17}\n",
      "(('perm_idx', 24), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 100), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model5_lgbm': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model4_xgb': 4.7725295101856255e-166, 'model_baseline': 4.7725295101856255e-166}\n",
      "(('perm_idx', 25), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 26), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 10), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model5_lgbm': 2.9368033185714593e-17, 'model_baseline': 2.9368033185714593e-17}\n",
      "(('perm_idx', 27), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 100), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model5_lgbm': 4.7725295101856255e-166, 'model_baseline': 4.7725295101856255e-166}\n",
      "(('perm_idx', 28), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 1), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 29), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 10), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model5_lgbm': 2.9368033185714593e-17, 'model_baseline': 2.9368033185714593e-17}\n",
      "(('perm_idx', 30), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 100), ('strategy', 'round-robin'), ('order', 'random'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model5_lgbm': 4.7725295101856255e-166, 'model_baseline': 4.7725295101856255e-166}\n",
      "(('perm_idx', 31), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 32), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 10), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model_baseline': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model4_xgb': 2.9368033185714593e-17, 'model5_lgbm': 2.9368033185714593e-17}\n",
      "(('perm_idx', 33), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.05), ('greed_factor', 100), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model_baseline': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model4_xgb': 4.7725295101856255e-166, 'model5_lgbm': 4.7725295101856255e-166}\n",
      "(('perm_idx', 34), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 35), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 10), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model_baseline': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model4_xgb': 2.9368033185714593e-17, 'model5_lgbm': 2.9368033185714593e-17}\n",
      "(('perm_idx', 36), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.1), ('greed_factor', 100), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model_baseline': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model4_xgb': 4.7725295101856255e-166, 'model5_lgbm': 4.7725295101856255e-166}\n",
      "(('perm_idx', 37), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 1), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model4_xgb': 0.9, 'model1_rf': 0.02, 'model2_lr': 0.02, 'model3_lr': 0.02, 'model5_lgbm': 0.02, 'model_baseline': 0.02}\n",
      "(('perm_idx', 38), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 10), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model_baseline': 0.9999999999999999, 'model1_rf': 2.9368033185714593e-17, 'model2_lr': 2.9368033185714593e-17, 'model3_lr': 2.9368033185714593e-17, 'model4_xgb': 2.9368033185714593e-17, 'model5_lgbm': 2.9368033185714593e-17}\n",
      "(('perm_idx', 39), ('policy', 'epsilon_greedy'), ('ucb_scale', None), ('epsilon', 0.25), ('greed_factor', 100), ('strategy', 'greedy'), ('order', 'best'))\n",
      "Allocations after round 50 :\n",
      " {'model_baseline': 1.0, 'model1_rf': 4.7725295101856255e-166, 'model2_lr': 4.7725295101856255e-166, 'model3_lr': 4.7725295101856255e-166, 'model4_xgb': 4.7725295101856255e-166, 'model5_lgbm': 4.7725295101856255e-166}\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "simulations = {}\n",
    "for _, v in permutations.items():\n",
    "    policy = v['policy']\n",
    "    if policy == 'UCB1':\n",
    "        for strategy, order in v['allocations']:\n",
    "            perm_idx = make_perm_idx(i, policy, None, None, None, strategy, order)\n",
    "            print(perm_idx)\n",
    "            simulations[perm_idx] = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                                       ucb_scale, epsilon, greed_factor, strategy, order)\n",
    "            i = i + 1\n",
    "    if policy == 'Bayes_UCB':\n",
    "        for strategy, order in v['allocations']:\n",
    "            for ucb_scale in v['options']['ucb_scale']:\n",
    "                perm_idx = make_perm_idx(i, policy, ucb_scale, None, None, strategy, order)\n",
    "                print(perm_idx)\n",
    "                simulations[perm_idx] = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                                           ucb_scale, epsilon, greed_factor, strategy, order)\n",
    "                i = i + 1\n",
    "    if policy == 'epsilon_greedy':\n",
    "        for strategy, order in v['allocations']:\n",
    "            for episilon in v['options']['episilon']:\n",
    "                for greed_factor in v['options']['greed_factor']:\n",
    "                    perm_idx = make_perm_idx(i, policy, None, episilon, greed_factor, strategy, order)\n",
    "                    print(perm_idx)\n",
    "                    simulations[perm_idx] = perform_simulation(allocation_pool_df, target_results_df, allocation_method, policy,\n",
    "                                                               ucb_scale, epsilon, greed_factor, strategy, order)\n",
    "                    i = i + 1                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_to_json(data):\n",
    "    if data is None or isinstance(data, (bool, int, str)):\n",
    "        return data\n",
    "    if isinstance(data, (tuple, frozenset)):\n",
    "        return str(data)\n",
    "    raise TypeError\n",
    "\n",
    "def to_json(data):\n",
    "    if data is None or isinstance(data, (bool, int, tuple, range, str, list)):\n",
    "        return data\n",
    "    if isinstance(data, (set, frozenset)):\n",
    "        return sorted(data)\n",
    "    if isinstance(data, dict):\n",
    "        return {key_to_json(key): to_json(data[key]) for key in data}\n",
    "    raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write simulation sequence to a json file\n",
    "with open('simulations.json', 'w') as json_file:\n",
    "    json.dump(to_json(simulations), json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:min_ds] *",
   "language": "python",
   "name": "conda-env-min_ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
